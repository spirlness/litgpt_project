model_name: MoE-200M
model_config:
  bias: false
  block_size: 2048
  mlp_class_name: LLaMAMoE
  moe_intermediate_size: 2048
  n_embd: 768
  n_expert: 8
  n_expert_per_token: 2
  n_head: 12
  n_layer: 12
  n_query_groups: 4
  name: MoE-200M
  norm_class_name: RMSNorm
  norm_eps: 0.00001
  padded_vocab_size: 50257
  parallel_residual: false
  rope_base: 10000
  vocab_size: 50257
out_dir: ./checkpoints
precision: bf16-mixed
tokenizer_dir: ./data/tokenizer
data:
  class_path: litgpt.data.TextFiles
  init_args:
    train_data_path: ./data/custom_text/train
    val_data_path: ./data/custom_text/val
    num_workers: 2
train:
  global_batch_size: 16
  log_interval: 1
  max_tokens: 320000
  lr_warmup_steps: 5
  micro_batch_size: 2
  save_interval: 10
logger_name: csv
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0003
    weight_decay: 0.01
