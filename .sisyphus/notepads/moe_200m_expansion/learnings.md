2026-01-24
- Current scripts: create_litgpt_config.py writes litgpt_config.yaml with global/micro batch sizes; prepare_data.py downloads GPT-2 tokenizer and TinyStories via litgpt.
- Model config in configs/moe_200m.yaml defines 8 experts with 2 active, 12-layer 768d model.
