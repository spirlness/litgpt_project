# TPU v6e-1 optimized config
out_dir: ./checkpoints
precision: bf16-true  # TPU supports native BF16, which is more stable than FP16
tokenizer_dir: ./data/tokenizer

# TPU settings
accelerator: tpu
devices: 1  # v6e-1 is a single device slice
num_nodes: 1

data:
  class_path: litgpt.data.TextFiles
  init_args:
    train_data_path: ./data/custom_text/train
    val_data_path: ./data/custom_text/val
    num_workers: 0  # XLA sometimes dislikes multiprocessing dataloaders, 0 is safest

train:
  global_batch_size: 128
  log_interval: 1
  max_tokens: 1000000000
  lr_warmup_steps: 10
  micro_batch_size: 8  # TPU v6e has high HBM, we can likely fit 8 or more
  save_interval: 1000
  gradient_checkpointing: true
  max_norm: 1.0
  max_seq_length: 512

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0003
    weight_decay: 0.01
