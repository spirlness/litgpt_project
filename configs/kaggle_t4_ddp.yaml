# T4 x2 (15GB each) DDP config for Kaggle
out_dir: ./checkpoints
precision: 16-mixed
tokenizer_dir: ./data/tokenizer

# DDP settings for multi-GPU on Kaggle
strategy: ddp
devices: 2
num_nodes: 1

data:
  class_path: litgpt.data.TextFiles
  init_args:
    train_data_path: ./data/custom_text/train
    val_data_path: ./data/custom_text/val
    num_workers: 2

train:
  global_batch_size: 128
  log_interval: 1
  max_tokens: 1000000000
  lr_warmup_steps: 10
  micro_batch_size: 4
  save_interval: 250
  gradient_checkpointing: true
  max_norm: 1.0
  max_seq_length: 512

eval:
  interval: 0
  final_validation: false

resume: auto

checkpointing:
  upload_to_hf: false
  # hf_repo_id: your-username/your-model-repo

# Performance optimization options
optimization:
  # torch.compile settings
  compile: false
  compile_mode: reduce-overhead
  compile_dynamic: true
  compile_fullgraph: false

  # Flash Attention settings (T4 supports SM 7.5, Flash Attention 2 needs SM 8.0+)
  flash_attention: false
  flash_attention_force: false
  disable_math_fallback: false

logger_name: csv

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0003
    weight_decay: 0.01
