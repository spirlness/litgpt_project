model_name: MoE-30M-Debug
model_config:
  bias: false
  block_size: 1024
  mlp_class_name: LLaMAMoE
  moe_intermediate_size: 512
  n_embd: 256
  n_expert: 4
  n_expert_per_token: 2
  n_head: 4
  n_layer: 4
  n_query_groups: 4
  name: MoE-30M-Debug
  norm_class_name: RMSNorm
  norm_eps: 0.00001
  padded_vocab_size: 50257

  parallel_residual: false
  rope_base: 10000
  vocab_size: 50257
out_dir: ./checkpoints
precision: bf16-mixed
tokenizer_dir: ./data/tokenizer
data:
  class_path: litgpt.data.TextFiles
  init_args:
    train_data_path: ./data/custom_text/train
    val_data_path: ./data/custom_text/val
    num_workers: 2
train:
  global_batch_size: 16
  log_interval: 1
  max_tokens: 320000
  lr_warmup_steps: 2
  micro_batch_size: 4
  save_interval: 10
logger_name: csv

