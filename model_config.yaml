# Model configuration
name: MoE-200M
bias: false
block_size: 2048
mlp_class_name: LLaMAMoE
moe_intermediate_size: 2048
n_embd: 768
n_expert: 8
n_expert_per_token: 2
n_head: 12
n_layer: 12
n_query_groups: 4
norm_class_name: RMSNorm
norm_eps: 0.00001
padded_vocab_size: 50257
parallel_residual: false
rope_base: 10000
vocab_size: 50257
